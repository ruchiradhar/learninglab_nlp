{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94799da-4815-4059-8bd7-582c89fc925c",
   "metadata": {},
   "source": [
    "# Representing Text for NLP\n",
    "\n",
    "In any field of machine learning, we pass in our input to the model as numbers or vectors. This is a bit problematic when it comes to text and it needs to be converted into a numerical representation. These numerical representations of text are called encodings or embeddings.\n",
    "\n",
    "There are multiple ways of creating embeddings from any given text. In this notebook, we will explore some of the most common methods: \n",
    "\n",
    "1. Bag of Words (BOW)\n",
    "2. Word2Vec CBOW\n",
    "3. Word2Vec Skipgram\n",
    "4. GloVe\n",
    "5. FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da444925-93b0-4d95-b399-afb850875313",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "The bag of words method converts a text into a embedding based on a measure of occurence. The word \"bag\" here indicates that there is no order or position information for tokens involved in our vectors. For example: \"John loves Mary\" and \"Mary loves John\" have the same embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "faf17d00-24cb-4a5d-b852-7d4992aedac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english') \n",
    "from nltk.stem import PorterStemmer  \n",
    "import string \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba79f79b-ba4c-4142-b31d-5e3175d90adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text string: 438807\n",
      "Num tokens: 42316\n",
      "Vocab size: 7635\n",
      "Num docs: 85\n",
      "Document-term matrix shape: (85, 7635)\n",
      "The embedding for doc 1:  [0 0 0 ... 0 0 0]\n",
      "chapter 24\n",
      "i 16\n",
      "may 9\n",
      "letter 6\n",
      "ebook 6\n",
      "frankenstein 4\n",
      "modern 4\n",
      "prometheus 4\n",
      "the 4\n",
      "these 4\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words Model\n",
    "# uploading text\n",
    "with open (\"pg84.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    text=f.read()\n",
    "print(\"Length of text string:\", len(text))\n",
    "\n",
    "# preprocess text\n",
    "def preprocess_text(text): \n",
    "    tokens= word_tokenize(text)\n",
    "    tokens= [token for token in tokens if (token not in stopwords_english and token not in string.punctuation)]\n",
    "    tokens= [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "tokens=preprocess_text(text)\n",
    "print(\"Num tokens:\", len(tokens))\n",
    "\n",
    "# get vocabulary\n",
    "vocab=list(set(tokens))\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# split into documents/sentences\n",
    "doc_size=500\n",
    "docs= [tokens[i:i+doc_size] for i in range (0, len(tokens),doc_size)]\n",
    "print(\"Num docs:\", len(docs))\n",
    "\n",
    "# create the BOW vector \n",
    "# we use frequency counts i.e how many times a word occurs in a doc\n",
    "X= np.zeros((len(docs), len(vocab)), dtype=np.int32) #the empty embedding matrix where each doc is a row\n",
    "vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "for i,doc in enumerate(docs): \n",
    "    for word in doc:\n",
    "        j=vocab_index[word]\n",
    "        X[i,j]+=1 #each row in X is now an embedding for a doc\n",
    "print(\"Document-term matrix shape:\", X.shape)  # (num_docs, vocab_size)   \n",
    "\n",
    "# explore top words in doc 1\n",
    "row=X[0]\n",
    "top_idx=row.argsort()[::-1][:10]\n",
    "print(\"The embedding for doc 1: \", row)\n",
    "for item in top_idx:\n",
    "    print(vocab[item], row[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c15c8-287a-4c9f-aa64-12f34d1b348b",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "The Word2Vec method was first proposed in [Mikolov et al (2013)](https://openreview.net/forum?id=idpCdOWtqXd60) for models to learn word representations from a large corpus. The most common software implementation is available via `models.word2vec` from [Gensim](https://radimrehurek.com/gensim/models/word2vec.html). There are two methods in this: \n",
    "\n",
    "1. Continuous Bag of Words (CBOW): Given the context terms within a preselected context window, the model learns to predict the target term.\n",
    "2. Skipgram: Given the target word, the model tries to predict context words within the context window. All context words are intended to have higher probability compared to other words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454d6fa-00f5-438e-a701-32db359d7811",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Word2Vec CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a256ebe-d9dd-430e-8e3b-bc3b160559fb",
   "metadata": {},
   "source": [
    "### Word2Vec Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438efc8-ce7d-45c4-a041-8d86941d92e7",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "The GloVe or Global Vectors method was proposed in [Pennington et al (2014)](https://aclanthology.org/D14-1162/) and  aims to improve upon word representations by also including global context while learning the representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71105e11-c5b4-424f-a6b1-74fdf847f00a",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "The FasText method was first proposed in [Bojanowski et al (2017)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00051/43387/Enriching-Word-Vectors-with-Subword-Information). Unlike using words as the unit of representation, it decomposes each word into n-grams to learn representations of these different character combinations. \n",
    "\n",
    "One of the major advantages of this method was the ability to represent out of vocabulary (OOV) tokens by some mathematical function of the subword ngrams for the token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff1380-4cfe-4398-93b5-cafd036edfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

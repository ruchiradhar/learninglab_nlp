{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94799da-4815-4059-8bd7-582c89fc925c",
   "metadata": {},
   "source": [
    "# Representing Text for NLP\n",
    "\n",
    "In any field of machine learning, we pass in our input to the model as numbers or vectors. This is a bit problematic when it comes to text and it needs to be converted into a numerical representation. These numerical representations of text are called encodings or embeddings.\n",
    "\n",
    "There are multiple ways of creating embeddings from any given text. In this notebook, we will explore some of the most common methods: \n",
    "\n",
    "1. Bag of Words (BOW)\n",
    "2. Word2Vec CBOW\n",
    "3. Word2Vec Skipgram\n",
    "4. GloVe\n",
    "5. FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da444925-93b0-4d95-b399-afb850875313",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "The bag of words method converts a text into a embedding based on a measure of occurence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c15c8-287a-4c9f-aa64-12f34d1b348b",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "The Word2Vec method was first proposed in [Mikolov et al (2013)](https://openreview.net/forum?id=idpCdOWtqXd60) for models to learn word representations from a large corpus. The most common software implementation is available via `models.word2vec` from [Gensim](https://radimrehurek.com/gensim/models/word2vec.html). There are two methods in this: \n",
    "\n",
    "1. Continuous Bag of Words (CBOW): Given the context terms within a preselected context window, the model learns to predict the target term.\n",
    "2. Skipgram: Given the target word, the model tries to predict context words within the context window. All context words are intended to have higher probability compared to other words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454d6fa-00f5-438e-a701-32db359d7811",
   "metadata": {},
   "source": [
    "### Word2Vec CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a256ebe-d9dd-430e-8e3b-bc3b160559fb",
   "metadata": {},
   "source": [
    "### Word2Vec Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438efc8-ce7d-45c4-a041-8d86941d92e7",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "The GloVe or Global Vectors method was proposed in [Pennington et al (2014)](https://aclanthology.org/D14-1162/) and  aims to improve upon word representations by also including global context while learning the representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71105e11-c5b4-424f-a6b1-74fdf847f00a",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "The FasText method was first proposed in [Bojanowski et al (2017)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00051/43387/Enriching-Word-Vectors-with-Subword-Information). Unlike using words as the unit of representation, it decomposes each word into n-grams to learn representations of these different character combinations. \n",
    "\n",
    "One of the major advantages of this method was the ability to represent out of vocabulary (OOV) tokens by some mathematical function of the subword ngrams for the token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff1380-4cfe-4398-93b5-cafd036edfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
